# -*- coding: utf-8 -*-
"""merged_Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SO4d1KL2q8sT8-t7katjS9YpjsfRPY4l
"""



import nltk

from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet, stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline
from tqdm import tqdm
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import softmax
import json
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
import dask.dataframe as dd

reviews = []
with open('/content/drive/MyDrive/Video_Games_5.json', 'r') as file:
    for line in file:
        review = json.loads(line)
        reviews.append(review)

data = pd.DataFrame(reviews)
data.to_csv('reviews.csv', index=False)

# Load data from 'fashion_reviews.csv'
df = pd.read_csv('reviews.csv')
df = df.head(3000)

def clean_text(text):
    tokenizer = RegexpTokenizer(r"\w+")
    word_tokens = tokenizer.tokenize(text)
    stop_words = set(stopwords.words("english"))
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    ps = PorterStemmer()
    stemmed_sentence = [ps.stem(w) for w in filtered_sentence]
    tagged_sentence = pos_tag(filtered_sentence)  # Use pos_tag to generate (word, tag) tuples
    named_entities = nltk.ne_chunk(tagged_sentence)  # Pass the tagged sentence to ne_chunk
    lemmatizer = WordNetLemmatizer()
    return stemmed_sentence, tagged_sentence, named_entities, lemmatizer

# Sentiment Analysis using Vader
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()


def vader_sentiment(text):
    if isinstance(text, str):
        return sia.polarity_scores(text)
    else:
        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}

# Sentiment Analysis using RoBERTa
MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)


# ... (previous code remains the same)

def polarity_scores_roberta(review_text):
    # Limit the text length to avoid potential errors
    max_length = 512  # Define the maximum length for BERT input

    # Truncate or split the text if it's too long
    text = review_text[:max_length] if len(review_text) > max_length else review_text

    # Get BERT embeddings for the processed text
    encoded_text = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length)

    # Use the model to get the sentiment scores
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    return {
        'roberta_neg': scores[0],
        'roberta_neu': scores[1],
        'roberta_pos': scores[2]
    }

# ... (rest of your code remains the same)



# Merging functionalities
nltk.download('words')
results = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        text = row["reviewText"]

        # Check if the text is a valid string
        if isinstance(text, str) and len(text) > 0:
            stemmed_sentence, tagged_sentence, named_entities, lemmatizer = clean_text(text)
            vader_result = vader_sentiment(text)
            roberta_result = polarity_scores_roberta(text)
            results[i] = {
                'stemmed_sentence': stemmed_sentence,
                'tagged_sentence': tagged_sentence,
                'named_entities': named_entities,
                'lemmatizer': lemmatizer,
                **vader_result,
                **roberta_result
            }
        else:
            # Handle empty or non-string text data
            print(f"Skipping empty or non-string text for index {i}")
    except RuntimeError:
        print(f'Broke for id {i}')

merged_results_df = pd.DataFrame(results).T
merged_results_df.reset_index(inplace=True)
merged_results_df = merged_results_df.merge(df, left_index=True, right_index=True, suffixes=('_merged', '_original'))
# Add topic distribution to your DataFrame

from sklearn.feature_extraction.text import TfidfVectorizer

# Filter out rows with missing reviewText
merged_results_df = merged_results_df.dropna(subset=['reviewText'])

# Separate highly positive and highly negative reviews (assuming 4-5 stars as highly positive and 1-2 stars as highly negative)
highly_positive_reviews = merged_results_df[merged_results_df['overall'] >= 4]['reviewText']
highly_negative_reviews = merged_results_df[merged_results_df['overall'] <= 2]['reviewText']

# Define a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed

# Fit and transform the highly positive reviews
tfidf_positive = tfidf_vectorizer.fit_transform(highly_positive_reviews)

# Get feature names (words)
positive_feature_names = tfidf_vectorizer.get_feature_names_out()

# Fit and transform the highly negative reviews
tfidf_negative = tfidf_vectorizer.fit_transform(highly_negative_reviews)

# Get feature names (words)
negative_feature_names = tfidf_vectorizer.get_feature_names_out()

# Extract top keywords for highly positive and highly negative reviews
def get_top_keywords(tfidf_matrix, features, top_n=10):
    # Calculate TF-IDF scores for each word in the matrix
    scores = tfidf_matrix.sum(axis=0)

    # Get indices of top features
    top_indices = scores.argsort()[0, -top_n:]

    # Get top words and their scores
    top_words = [(features[i], scores[0, i]) for i in top_indices]

    return sorted(top_words, key=lambda x: x[1], reverse=True)

# Get top keywords for highly positive reviews
top_positive_keywords = get_top_keywords(tfidf_positive, positive_feature_names)

# Get top keywords for highly negative reviews
top_negative_keywords = get_top_keywords(tfidf_negative, negative_feature_names)

# Print top keywords
print("Top keywords in highly positive reviews:")
for keyword, score in top_positive_keywords:
    print(f"{keyword}: {score}")

print("\nTop keywords in highly negative reviews:")
for keyword, score in top_negative_keywords:
    print(f"{keyword}: {score}")

# Further analysis and visualization can be done using merged_results_df
# Example: Visualizing sentiment scores against Amazon review ratings

sns.pairplot(data=merged_results_df,
             vars=['neg', 'neu', 'pos', 'roberta_neg', 'roberta_neu', 'roberta_pos'],
             hue='overall',
             palette='tab10')
plt.show()

fig, axs = plt.subplots(1,3, figsize=(15, 5))
sns.barplot(data=merged_results_df, x='overall', y="pos", ax=axs[0])
sns.barplot(data=merged_results_df, x='overall', y="neg", ax=axs[1])
sns.barplot(data=merged_results_df, x='overall', y="neu", ax=axs[2])

axs[0].set_title('V_Positive')
axs[1].set_title('V_Negative')
axs[2].set_title('V_Neutral')
plt.tight_layout()
plt.show()

fig, axs = plt.subplots(1,3, figsize=(15, 5))
sns.barplot(data=merged_results_df, x='overall', y="roberta_pos", ax=axs[0])
sns.barplot(data=merged_results_df, x='overall', y="roberta_neg", ax=axs[1])
sns.barplot(data=merged_results_df, x='overall', y="roberta_neu", ax=axs[2])

axs[0].set_title('R_Positive')
axs[1].set_title('R_Negative')
axs[2].set_title('R_Neutral')
plt.tight_layout()
plt.show()

merged_results_df.iloc[1]['reviewText']

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate average ratings for each product
average_ratings = merged_results_df.groupby('asin')['overall'].mean().reset_index()

# Sort to find highly rated products
highly_rated_products = average_ratings.sort_values(by='overall', ascending=False)

# Sort to find poorly rated products
poorly_rated_products = average_ratings.sort_values(by='overall')

# Define a high-rated ASIN for analysis
high_rated_asin = highly_rated_products.iloc[0]['asin']  # Assuming the top-rated ASIN is used

# Filter reviews based on high_rated_asin
high_rated_reviews = merged_results_df[merged_results_df['asin'] == high_rated_asin]

# Separate high and low-rated reviews for comparison
high_rated = high_rated_reviews[high_rated_reviews['overall'] >= 4]  # Define high ratings (4 and 5 stars)
low_rated = high_rated_reviews[high_rated_reviews['overall'] <= 2]   # Define low ratings (1 and 2 stars)

# Create histograms for sentiment scores of high-rated and low-rated items
plt.figure(figsize=(10, 6))

# Vader sentiment scores
sns.histplot(high_rated['compound'], label='High Ratings', kde=True)
sns.histplot(low_rated['compound'], label='Low Ratings', kde=True)

plt.xlabel('Sentiment Score (Vader - Compound)')
plt.ylabel('Frequency')
plt.title('Sentiment Distribution for High and Low Rated Items (Vader)')
plt.legend()
plt.show()

# Assuming 'reviewTime' is in a datetime format
merged_results_df['reviewTime'] = pd.to_datetime(merged_results_df['reviewTime'])

# Group by reviewTime and calculate mean sentiment scores
sentiment_over_time = merged_results_df.groupby(['reviewTime'])['compound'].mean()

# Plot sentiment trends over time
plt.figure(figsize=(12, 6))
sentiment_over_time.plot()
plt.xlabel('Time')
plt.ylabel('Mean Sentiment Score (Vader - Compound)')
plt.title('Sentiment Trends Over Time')
plt.show()

print("Top 10 Highly Rated Products:")
highly_rated_products.head(10)

print("\nTop 10 Poorly Rated Products:")
poorly_rated_products.head(10)

# Define a poorly rated ASIN for analysis (select the lowest rated product)
poorly_rated_asin = poorly_rated_products.iloc[0]['asin']  # Assuming the lowest-rated ASIN is used

# Filter reviews based on poorly_rated_asin
poorly_rated_reviews = merged_results_df[merged_results_df['asin'] == poorly_rated_asin]

# Separate high and low-rated reviews for comparison
high_rated_poorly = poorly_rated_reviews[poorly_rated_reviews['overall'] >= 4]  # Define high ratings (4 and 5 stars)
low_rated_poorly = poorly_rated_reviews[poorly_rated_reviews['overall'] <= 2]   # Define low ratings (1 and 2 stars)

# Create histograms for sentiment scores of high-rated and low-rated items
plt.figure(figsize=(10, 6))

# Vader sentiment scores
sns.histplot(high_rated_poorly['compound'], label='High Ratings', kde=True)
sns.histplot(low_rated_poorly['compound'], label='Low Ratings', kde=True)

plt.xlabel('Sentiment Score (Vader - Compound)')
plt.ylabel('Frequency')
plt.title('Sentiment Distribution for High and Low Rated Items (Vader) - Poorly Rated Product')
plt.legend()
plt.show()

# Assuming 'reviewTime' is in a datetime format
merged_results_df['reviewTime'] = pd.to_datetime(merged_results_df['reviewTime'])

# Group by reviewTime and calculate mean sentiment scores
sentiment_over_time_poorly = poorly_rated_reviews.groupby(['reviewTime'])['compound'].mean()

# Plot sentiment trends over time for the poorly rated product
plt.figure(figsize=(12, 6))
sentiment_over_time_poorly.plot()
plt.xlabel('Time')
plt.ylabel('Mean Sentiment Score (Vader - Compound)')
plt.title('Sentiment Trends Over Time - Poorly Rated Product')
plt.show()